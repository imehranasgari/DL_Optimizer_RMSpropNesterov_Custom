# RMSprop with Nesterov Momentum â€“ Custom Optimizer Implementation

## 1) Project Title

**Custom Implementation of RMSprop with Nesterov Momentum**

---

## 2) Problem Statement and Goal of Project

Optimizers play a critical role in controlling the convergence speed and stability of training deep learning models. While many optimizers are readily available in frameworks like TensorFlow and PyTorch, implementing them manually deepens understanding of their mechanics.
The goal of this project is to:

* Implement a **custom RMSprop optimizer** enhanced with **Nesterov momentum** from scratch.
* Provide a minimal, framework-independent base for experimentation and visualization.

---

## 3) Solution Approach

* **Custom Base Class (`Optimizer`)**

  * Defines the API (`update`) that all derived optimizers must implement.
  * Stores `learning_rate` and uses type annotations for clarity.

* **`RMSpropNesterov` Implementation**

  * Incorporates:

    * **Exponential moving average of squared gradients** (RMSprop behavior)
    * **Lookahead gradient evaluation** (Nesterov momentum)
    * Configurable hyperparameters: `decay_rate`, `momentum`, `epsilon`.

* **Function & Gradient Example**

  * Example function `f(x) = xÂ²` and gradient `f'(x) = 2x` provided for testing optimizer behavior.

* **Visualization Ready**

  * Includes `matplotlib` imports (2D & 3D plotting support in `project02`) for potential trajectory visualization of parameter updates.

---

## 4) Technologies & Libraries

* **Core**: Python 3.x
* **Numerical Computation**: NumPy
* **Visualization**: Matplotlib (`Axes3D` in `project02` for 3D plots)

---

## 5) Description about Dataset

**Not applicable** â€“ This project focuses on optimizer implementation and does not use a dataset.

---

## 6) Installation & Execution Guide

**Prerequisites**

```bash
pip install numpy matplotlib
```

**Run**

1. Open `project01.ipynb` or `project02.ipynb` in Jupyter Notebook or JupyterLab.
2. Inspect the optimizer implementation.
3. Replace the example function/gradient with your own to test on different optimization landscapes.
4. (Optional) Extend with plotting code to visualize optimizer trajectories.

---

## 7) Key Results / Performance

* Successfully implements a fully functional **RMSprop with Nesterov Momentum** in pure Python/NumPy.
* Modular design allows:

  * Easy swapping of the objective function.
  * Extension to other optimizers by subclassing `Optimizer`.
* Ready for integration into lightweight ML pipelines or educational demos.

---

## 8) Screenshots / Sample Output

*(No visualization code included in the provided notebooks; plots can be generated by adding Matplotlib trajectory drawing using stored parameter states.)*

---

## 9) Additional Learnings / Reflections

* Writing an optimizer from scratch reinforces understanding of gradient-based optimization internals.
* Separating the base `Optimizer` class promotes cleaner, reusable code.
* The combination of RMSprop and Nesterov offers benefits in both convergence speed and stability, making it a valuable optimizer variant to understand and experiment with.

---

## ðŸ‘¤ Author

**Mehran Asgari**
**Email:** [imehranasgari@gmail.com](mailto:imehranasgari@gmail.com)
**GitHub:** [https://github.com/imehranasgari](https://github.com/imehranasgari)

---

## ðŸ“„ License

This project is licensed under the **Apache 2.0 License** â€“ see the `LICENSE` file for details.

---

> ðŸ’¡ *Some interactive outputs (e.g., plots, widgets) may not display correctly on GitHub. If so, please view this notebook via [nbviewer.org](https://nbviewer.org) for full rendering.*

---
